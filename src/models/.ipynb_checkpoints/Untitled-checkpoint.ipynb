{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Train Module for NFL Play Prediction Project\n",
    "@author: Dennis Tseng\n",
    "\n",
    "Part of the DSC 672 Capstone Final Project Class\n",
    "Group 3: Dennis Tseng, Scott Elmore, Dongmin Sun\n",
    "\n",
    "'Branched' from Casey Bennett's sklearn template. Modified by Dennis Tseng\n",
    "to allow for the use of pipelines and imblearn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# Load Required Libraries\n",
    "#\n",
    "#####################\n",
    "\n",
    "# Standard packages\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Pipeline, if not using imblearn comment that out and use sklearn's pipeline instead\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# Some sklearn tools for preprocessing, feature selection and etc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer, scale, MinMaxScaler\n",
    "from sklearn.feature_selection import RFE, RFECV, VarianceThreshold, SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, mutual_info_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imbalanced Datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost.sklearn as xgb\n",
    "from catboost import CatBoostClassifier, cv, Pool\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Model Exporting\n",
    "#from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "import lime\n",
    "\n",
    "# pip install tune-sklearn ray[tune]\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "# ML Flow\n",
    "#import mlflow.sklearn\n",
    "\n",
    "\n",
    "#Handle annoying warnings\n",
    "import warnings, sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\n",
    "\n",
    "\n",
    "# %% \n",
    "#############################################################################\n",
    "#\n",
    "# Global parameters\n",
    "#\n",
    "#####################\n",
    "\n",
    "imb_class=0                                         #Control switch for type of sampling to deal with imbalanced class (0=None, 1=SMOTE, 2=NearMiss, 3=RandomUnderSampler)\n",
    "cross_val=12                                         #Control Switch for CV\n",
    "norm_features=1                                     #Normalize features switch\n",
    "pca = 0\n",
    "feat_select=0                                       #Control Switch for Feature Selection\n",
    "fs_type=2                                           #Feature Selection type (1=Stepwise Backwards Removal, 2=Wrapper Select, 3=Univariate Selection)\n",
    "lv_filter=0                                         #Control switch for low variance filter on features\n",
    "k_cnt= 20                                           #Number of 'Top k' best ranked features to select, only applies for fs_types 1 and 3\n",
    "param_tuning = 1                                    #Turn on model parameter tuning\n",
    "exhaustive_search = 0                               #Turn on if you want exhaustive grid search. Otherwise, it will default to RandomizedSearchCV\n",
    "\n",
    "#Set global model parameters\n",
    "rand_st=0                                           #Set Random State variable for randomizing splits on runs\n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Load Data\n",
    "#\n",
    "#####################\n",
    "print('Loading Data...')\n",
    "start_ts=time.time()\n",
    "\n",
    "data = pd.read_csv(\"../../data/clean/model_plays.csv\", low_memory = False)\n",
    "data['target'].value_counts()\n",
    "\n",
    "# Remove labels\n",
    "del data['play_id']\n",
    "del data['game_id']\n",
    "\n",
    "\n",
    "# Result of Feature Selection - Other Script\n",
    "data.drop(['play_type_punt', 'Fog', 'ld_outcome_end_of_half', 'pd_expl_pass', 'play_type_field_goal', 'play_type_qb_kneel', 'GameMonth', 'Wind', \n",
    "           'ld_opp_outcome_end_of_half', 'ld_opp_outcome_punt', 'ld_outcome_field_goal', 'pd_average_tfl', 'Rain', 'Snow', 'def_fs', 'def_le',\n",
    "           'def_re', 'def_rolb', 'def_ss', 'fumble', 'home', 'ld_outcome_fumble_lost', 'ld_outcome_interception', 'ld_outcome_touchdown',\n",
    "           'off_lg', 'off_rg', 'off_te', 'def_cb', 'def_dt', 'def_mlb', 'ld_drive_length', 'ld_expl_pass', 'ld_plays', 'off_rt', 'pd_pass_yard_att',\n",
    "           'qb_scramble', 'def_lolb', 'ld_opp_outcome_field_goal', 'ld_opp_outcome_interception', 'ld_opp_outcome_no_ld', 'ld_outcome_no_ld',\n",
    "           'ld_outcome_punt', 'ld_outcome_turnover_on_downs', 'off_c', 'qb_spike', 'play_type_qb_spike' , 'pd_average_plays', 'pd_average_sacks', 'pd_average_top'\n",
    "           , 'pd_avg_interceptions'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Test for run and pass predictors. Comment out if we don't want this\n",
    "data = data[((data['target'] == 'run') |  (data['target'] == 'pass'))]\n",
    "\n",
    "# Separate Target from dataset\n",
    "#firstdata['target'] = data['target'].astype('category')\n",
    "#target_cat_label = dict(enumerate(data.target.categories))\n",
    "\n",
    "# Change categorical variables to numerical\n",
    "data['target'] = data['target'].astype('category').cat.codes\n",
    "# data['target'] = data['target'].map(target_cat_label)\n",
    "\n",
    "target = data['target']\n",
    "del data['target']\n",
    "\n",
    "# Split prior to any normalization/sampling. Stratify is set understanding that we have imbalanced classes and want to preserve that in the split\n",
    "sample_train, sample_test, sample_target_train, sample_target_test = train_test_split(data, target, test_size=0.50, stratify = target)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(sample_train, sample_target_train, test_size=0.50, stratify = sample_target_train)\n",
    "\n",
    "\n",
    "# Calculates Shannon Entropy - a measure of class imbalance \n",
    "# As suggested by Simone Romano, PhD. \n",
    "# https://stats.stackexchange.com/questions/239973/a-general-measure-of-data-set-imbalance\n",
    "def balance(seq):\n",
    "    n = len(seq)\n",
    "    classes = [(clas,float(count)) for clas,count in Counter(seq).items()]\n",
    "    k = len(classes)\n",
    "\n",
    "    H = -sum([ (count/n) * np.log((count/n)) for clas,count in classes]) #shannon entropy\n",
    "    return H/np.log(k)\n",
    "\n",
    "print('The balance of classes within the dataset is: ', balance(target_train))\n",
    "print('If this is far less than 1.0, please consider turning on the imblearn class balancing portions of the pipeline!')\n",
    "\n",
    "print()    \n",
    "print(\"Complete. Data Load Runtime:\", time.time()-start_ts)\n",
    "print()\n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Create pipeline lists \n",
    "#\n",
    "##########################################\n",
    "\n",
    "\n",
    "print('Creating Model Pipelines...')\n",
    "start_ts=time.time()\n",
    "\n",
    "# sklearn\n",
    "#knn_pipe = []\n",
    "#dt_pipe = []\n",
    "#rf_pipe = []\n",
    "#ab_pipe = []\n",
    "gb_pipe = []\n",
    "#svm_pipe = []\n",
    "#mlp_pipe = []\n",
    "#xg_pipe = []\n",
    "\n",
    "skpipes = [gb_pipe]\n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Pre-processing\n",
    "#\n",
    "##########################################\n",
    "\n",
    "print('Normalization Turned On')\n",
    "\n",
    "# In the future ColumnTransformers can be utilized... actually it will be once this pipeline is finished\n",
    "'''\n",
    "preprocessor = ColumnTransformer([(\"numerical\", \"passthrough\", num_features), \n",
    "                                  (\"categorical\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n",
    "                                   cat_features)])\n",
    "'''\n",
    "\n",
    "# Normalizing using Standard Scaler\n",
    "if norm_features == 1:\n",
    "    for i in range(0, len(skpipes)):\n",
    "        skpipes[i].append(('scalar'+str(i),StandardScaler()))\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "# Dealing with imbalanced classes\n",
    "if imb_class == 0:\n",
    "    pass\n",
    "elif imb_class == 1:\n",
    "    # Oversample with SMOTE\n",
    "    print('Balanced Classes Turned On')\n",
    "    for i in range(0, len(skpipes)):\n",
    "        skpipes[i].append(('smote'+str(i), SMOTE(random_state = rand_st)))\n",
    "        \n",
    "elif imb_class == 2:\n",
    "    # Undersample using NearMiss\n",
    "    print('Balanced Classes Turned On')\n",
    "    for i in range(0, len(skpipes)):\n",
    "        skpipes[i].append(('NearMiss'+str(i), NearMiss(version=3)))\n",
    "    \n",
    "elif imb_class == 3:\n",
    "    # Undersample using NearMiss\n",
    "    print('Balanced Classes Turned On')\n",
    "    for i in range(0, len(skpipes)):\n",
    "        skpipes[i].append(('undersample'+str(i), RandomUnderSampler()))\n",
    "    \n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Feature Selection\n",
    "#\n",
    "##########################################\n",
    "\n",
    "#Low Variance Filter\n",
    "if lv_filter==1:\n",
    "    print('Low Variance Filter Turned On', '\\n')\n",
    "    \n",
    "    for i in range(0, len(skpipes)):\n",
    "        skpipes[i].append(('variance_threshold'+str(i), VarianceThreshold(threshold = 0.5)))   \n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "if feat_select==1:\n",
    "    print('Feature Selection Turned On', '\\n')\n",
    "    \n",
    "    ##1) Run Feature Selection #######\n",
    "    if fs_type==1:\n",
    "        #Stepwise Recursive Backwards Feature removal\n",
    "        clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None, class_weight=\"balanced\")\n",
    "        cv_rfc = RFECV(clf, n_features_to_select=k_cnt, step=.1, cv = 5, scoring = 'roc_auc')\n",
    "            \n",
    "        for i in range(0, len(skpipes)):\n",
    "            skpipes[i].append(('rfe_rf'+str(i), cv_rfc))   \n",
    "        \n",
    "    if fs_type==2:\n",
    "        #Wrapper Select via model\n",
    "        clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "        sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "        \n",
    "        for i in range(0, len(skpipes)):\n",
    "            skpipes[i].append(('wrapper_rf'+str(i), sel))   \n",
    "\n",
    "    if fs_type==3:                                                                ######Only work if the Target is binned###########\n",
    "        #Univariate Feature Selection - Chi-squared\n",
    "        #will throw error if any negative values in features, so turn off feature normalization, or switch to mutual_info_classif\n",
    "        print ('Univariate Feature Selection - Chi2: ')\n",
    "        sel=SelectKBest(chi2, k=k_cnt)\n",
    "        \n",
    "        for i in range(0, len(skpipes)):\n",
    "            skpipes[i].append(('ufs'+str(i), sel)) \n",
    "\n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Dimensionality Reduction\n",
    "#\n",
    "##########################################\n",
    "\n",
    "# PCA Dimensionality Reduction\n",
    "if pca == 1:\n",
    "    print('PCA Turned On', '\\n')\n",
    "    for i in range(0, len(skpipes)):\n",
    "        skpipes[i].append(('pca'+str(i), PCA(n_components = 0.95)))    \n",
    "        \n",
    "\n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Add Classifiers\n",
    "#\n",
    "##########################################\n",
    "\n",
    "'''\n",
    "# Logistic Regression\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())\n",
    "                     ])\n",
    "'''\n",
    "\n",
    "# List of Classifiers\n",
    "classifiers = [#('knn_classifier', KNeighborsClassifier()),\n",
    "               \n",
    "               # Tree-based methods\n",
    "               #('dt_classifier', DecisionTreeClassifier()), \n",
    "               #('rf_classifier', RandomForestClassifier()),\n",
    "               #('ab_classifier', AdaBoostClassifier()),\n",
    "               \n",
    "               \n",
    "               # Support Vector Machines\n",
    "               #('sv_classifier', SVC()),\n",
    "               \n",
    "               # Neural Networks\n",
    "               #('nn_classifier', MLPClassifier()),\n",
    "               #('xg_classifier', xgb.XGBClassifier(tree_method = 'gpu_hist')),\n",
    "               #('xg_classifier', xgb.XGBClassifier()),\n",
    "               ('gb_classifier', GradientBoostingClassifier())\n",
    "               #\n",
    "               ]\n",
    "\n",
    "\n",
    "for i, c in enumerate(classifiers):\n",
    "    skpipes[i].append(c) \n",
    "\n",
    "# %%\n",
    "#############################################################################\n",
    "#\n",
    "# Create Pipelines\n",
    "#\n",
    "##########################################\n",
    "\n",
    "'''\n",
    "# Logistic Regression\n",
    "lr_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('classifier', LogisticRegression())\n",
    "                     ])\n",
    "'''\n",
    "\n",
    "# KNN\n",
    "#knn_pipe = Pipeline(skpipes[0])\n",
    "\n",
    "\n",
    "# Tree Methods\n",
    "#dt_pipe = Pipeline(skpipes[0])\n",
    "#rf_pipe = Pipeline(skpipes[0])\n",
    "#ab_pipe = Pipeline(skpipes[0])\n",
    "gb_pipe  = Pipeline(skpipes[0])\n",
    "#xg_pipe = Pipeline(skpipes[0])\n",
    "\n",
    "# SVMs\n",
    "#svm_pipe = Pipeline(skpipes[5])\n",
    "\n",
    "# Neural Networks\n",
    "#mlp_pipe = Pipeline(skpipes[4])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of all classifier pipelines\n",
    "pipelines = [gb_pipe]\n",
    "\n",
    "best_accuracy=0.0\n",
    "best_classifier=0\n",
    "best_pipeline=\"\"\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "pipe_dict = {1: 'Decision Tree', 2: 'RandomForest', 3: 'AdaBoost', 4:'GradientBoostedTrees', 5:'svm', 6:'feedforwardnn'}\n",
    "pipe_list = ['GradientBoostedTrees']\n",
    "\n",
    "print(\"Complete. Pipeline Runtime:\", time.time()-start_ts)\n",
    "print()\n",
    "\n",
    "\n",
    "# %% \n",
    "#############################################################################\n",
    "#\n",
    "# Hyperparameter Tuning\n",
    "#\n",
    "##########################################\n",
    "\n",
    "best_params = []\n",
    "best_estimator =[]\n",
    "\n",
    "  \n",
    "dt_param = [{'dt_classifier__criterion': ['gini', 'entropy'],\n",
    "             'dt_classifier__splitter': ['best', 'random'],\n",
    "             'dt_classifier__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "             'dt_classifier__max_depth': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "             'dt_classifier__min_samples_split': [2, 5, 10, 20, 25, 50, 100],\n",
    "             'dt_classifier__min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "             'dt_classifier__class_weight': ['balanced', 'balanced_subsample', None]}]\n",
    "\n",
    "rf_grid = [{'rf_classifier__n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)],\n",
    "            'rf_classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'rf_classifier__max_depth': [int(x) for x in np.linspace(1, 20, num = 20)],\n",
    "            'rf_classifier__min_samples_split': [2, 5, 10, 20, 25, 50, 100, 200],\n",
    "            'rf_classifier__min_samples_leaf': [2, 4, 8, 16, 32, 64, 128],\n",
    "            'rf_classifier__class_weight':  ['balanced', 'balanced_subsample', None],\n",
    "            'rf_classifier__bootstrap': [True, False]}]\n",
    "\n",
    "ab_grid = [{'ab_classifier__n_estimators': [int(x) for x in np.linspace(start = 50, stop = 1000, num = 20)],\n",
    "           'ab_classifier__learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "           'ab_classifier__algorithm': ['SAMME', 'SAMME.R']}]\n",
    "\n",
    "\n",
    "gb_grid = [{'gb_classifier__loss': ['deviance', 'exponential'],\n",
    "           'gb_classifier__learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "           'gb_classifier__n_estimators': [int(x) for x in np.linspace(start = 50, stop = 1000, num = 20)],\n",
    "           'gb_classifier__max_depth' : [3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "           'gb_classifier__max_features' : ['auto','sqrt','log2',None],\n",
    "           'gb_classifier__min_samples_split': [2, 5, 10, 20, 25, 50, 100],\n",
    "           'gb_classifier__min_samples_leaf': [1, 2, 4, 8, 16, 32, 64],\n",
    "           }]\n",
    "\n",
    "'''\n",
    "gb_grid = [{'gb_classifier__loss': ['deviance', 'exponential'],\n",
    "           'gb_classifier__learning_rate': (0.0001, 1),\n",
    "           'gb_classifier__n_estimators': (0, 1000),\n",
    "           'gb_classifier__max_depth' : (3,15),\n",
    "           'gb_classifier__max_features' : ['auto','sqrt','log2',None],\n",
    "           'gb_classifier__min_samples_split': (2, 100),\n",
    "           'gb_classifier__min_samples_leaf': (1, 128)\n",
    "           }]\n",
    "'''\n",
    "\n",
    "nn_grid = [{'nn_classifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "           'nn_classifier__hidden_layer_sizes' : [(100,), (10,), (50,50), (20,20), (100,50), (200,100), (30,30,30)],\n",
    "           'nn_classifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "           'nn_classifier__alpha' : [0.0001, 0.001, 0.01],\n",
    "           'nn_classifier__learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "           'nn_classifier__max_depth' : [3,4,5,6,7,8],\n",
    "           'nn_classifier__max_features' : ['auto','sqrt','log2',None],\n",
    "           'nn_classifier__min_samples_split': [2, 5, 10, 20, 25, 50, 100],\n",
    "           'nn_classifier__min_samples_leaf': [1, 2, 4, 8, 16, 32, 64],\n",
    "           }]\n",
    "\n",
    "\n",
    "xg_grid = [{#'xg_classifier__n_estimators': [int(x) for x in np.linspace(start = 1000, stop = 1500, num = 6)],\n",
    "            'xg_classifier__n_estimators': [1400],\n",
    "            #'xg_classifier__learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.150, 0.25, 0.5, 1],\n",
    "            'xg_classifier__learning_rate': [0.01],\n",
    "            'xg_classifier__min_split_loss': [2],\n",
    "            #'xg_classifier__min_child_weight': [0, 1, 3, 5, 7],\n",
    "            'xg_classifier__min_child_weight': [7],\n",
    "            #'xg_classifier__gamma': [i/10.0 for i in range(0,5)],\n",
    "            'xg_classifier__gamma': [0],\n",
    "            #'xg_classifier__max_depth': [3, 5, 7, 9, 11, 13,15, 17, 19, 21, 25],\n",
    "            'xg_classifier__max_depth': [7],\n",
    "            #'xg_classifier__booster': ['gbtree', 'gblinear', 'dart'],\n",
    "            'xg_classifier__booster': ['gbtree'],\n",
    "            #'xg_classifier__reg_alpha': [0, 0.001, 0.005, 0.1, 0.05, 1e-5, 0.1, 0.25, 0.5, 1, 1.25],\n",
    "            'xg_classifier__reg_alpha': [0.05],\n",
    "            'xg_classifier__reg_lambda': [0, 1, 1.1, 1.2, 1.3],\n",
    "            #'xg_classifier__sampling_method' : ['gradient_based', 'uniform'],\n",
    "            'xg_classifier__subsample': [0.9],\n",
    "            #'xg_classifier__subsample': [0.8],\n",
    "            'xg_classifier__colsample_bytree' : [ 0.6]\n",
    "            #'xg_classifier__colsample_bytree' : [0.8]\n",
    "            #'xg_classifier__early_stopping_rounds' : [50],\n",
    "            #'xg_classifier__eval_metric' : ['auc'],\n",
    "            #'xg_classifier__eval_set' : [[data_test, target_test]]\n",
    "           }]\n",
    "\n",
    "\n",
    "params = [gb_grid]\n",
    "\n",
    "if param_tuning == 1:\n",
    "    \n",
    "    gridhistory = []\n",
    "    \n",
    "    #scorers = {'AUC' : 'roc_auc'}\n",
    "    \n",
    "    for pipe, grid_param, name in zip(pipelines, params, pipe_list):\n",
    "        print('Tuning Models...')\n",
    "        start_ts=time.time()\n",
    "        if exhaustive_search == 1:\n",
    "            #gridsearch = GridSearchCV(pipe, grid_param, scoring = 'roc_auc', cv=5, verbose=10, n_jobs=-1) \n",
    "            gridsearch = TuneGridSearchCV(pipe, grid_param, scoring = 'roc_auc', cv=5, verbose=10, n_jobs=-1) \n",
    "        else:    \n",
    "            #gridsearch = RandomizedSearchCV(pipe, param_distributions = grid_param, n_iter=5000, scoring = 'roc_auc', cv=5, verbose=20, n_jobs=-1)\n",
    "            gridsearch = TuneSearchCV(pipe, param_distributions = grid_param, n_iter = 1000, scoring = 'roc_auc', search_optimization='random', verbose=2, n_jobs = -1)\n",
    "        gridsearch.fit(data_train, target_train)\n",
    " \n",
    "        # Get best results        \n",
    "        best_params.append((name, gridsearch.best_params_, gridsearch.best_score_))\n",
    "        print(gridsearch.best_params_, file=open('output.txt', 'a'))\n",
    "        print(gridsearch.best_score_, file=open('output.txt', 'a'))\n",
    "        \n",
    "        # Save optimized model\n",
    "        gridhistory.append(gridsearch.cv_results_)\n",
    "        \n",
    "        filename = name + '.sav'\n",
    "        best_model = gridsearch.best_estimator_\n",
    "        pickle.dump(best_model, open(filename, 'wb'))\n",
    "        best_estimator.append(gridsearch.best_estimator_)\n",
    "        \n",
    "        print(\"Complete. GridSearchCV Runtime:\", time.time()-start_ts)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
